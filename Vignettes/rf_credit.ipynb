{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeremygoldwasser/opt/anaconda3/envs/shap/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sage\n",
    "\n",
    "import sys\n",
    "sys.path.append('../HelperFiles/')\n",
    "from helper import *\n",
    "from helper_dep import *\n",
    "from helper_indep import *\n",
    "from helper_shapley_sampling import *\n",
    "from helper_kshap import *\n",
    "from os.path import join\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "data_path = \"../Simulations/Data/bank\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German Credit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sage.datasets.credit()\n",
    "# Property, other installment, housing, job, status of checking act, credit history, purpose, savings, employment since, marital status, old debtors\n",
    "n = df.shape[0]\n",
    "X_df = df.drop([\"Good Customer\"], axis=1)\n",
    "y = df[\"Good Customer\"]\n",
    "\n",
    "categorical_columns = [\n",
    "    'Checking Status', 'Credit History', 'Purpose', #'Credit Amount', # It's listed but has 923 unique values\n",
    "    'Savings Account/Bonds', 'Employment Since', 'Personal Status',\n",
    "    'Debtors/Guarantors', 'Property Type', 'Other Installment Plans',\n",
    "    'Housing Ownership', 'Job', #'Telephone', 'Foreign Worker' # These are just binary\n",
    "]\n",
    "X_binarized = pd.get_dummies(X_df, columns=categorical_columns)\n",
    "\n",
    "mapping_dict = {}\n",
    "for i, col in enumerate(X_df.columns):\n",
    "    bin_cols = []\n",
    "    for j, bin_col in enumerate(X_binarized.columns):\n",
    "        if bin_col.startswith(col):\n",
    "            bin_cols.append(j)\n",
    "    mapping_dict[i] = bin_cols\n",
    "\n",
    "np.random.seed(1)\n",
    "X_norm = (X_binarized-X_binarized.min())/(X_binarized.max()-X_binarized.min())\n",
    "n_train = round(n*0.8)\n",
    "train_idx = np.random.choice(n, n_train, replace=False)\n",
    "X_train, y_train = X_norm.iloc[train_idx].to_numpy(), y.iloc[train_idx].to_numpy()\n",
    "test_idx = np.setdiff1d(np.arange(n),train_idx)\n",
    "X_test, y_test = X_norm.iloc[test_idx].to_numpy(), y.iloc[test_idx].to_numpy()\n",
    "d = X_train.shape[1] # dimension of binarized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute standard deviations of features\n",
    "sds = []\n",
    "for i in range(d):\n",
    "    uu = np.unique(X_train[:,i])\n",
    "    if len(uu) == 2:\n",
    "        sds.append(uu)\n",
    "    else:\n",
    "        sds.append(np.repeat(np.std(X_train[:,i]),2))\n",
    "sds = np.array(sds)\n",
    "\n",
    "# Compute mean and covariance of training data\n",
    "feature_means = np.mean(X_train, axis=0)\n",
    "cov_mat = np.cov(X_train, rowvar=False)\n",
    "cov2 = correct_cov(cov_mat) # Recondition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class imbalance: 72.0\n",
      "Estimation accuracy: 76.0\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=1000).fit(X_train, y_train)\n",
    "print(\"Class imbalance: {}\".format(100*(max(np.mean(y_test), 1-np.mean(y_test)))))\n",
    "print(\"Estimation accuracy: {}\".format(np.mean((rf.predict(X_test) > 0.5)==y_test)*100))\n",
    "\n",
    "def fmodel(x):\n",
    "    return rf.predict_proba(x)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ControlSHAP values, assuming independent features\n",
    "### Compute true SHAP values of Taylor approximation around $x$, and verify $\\sum_{j=1}^d \\phi_j(x) \\approx f(x)-Ef(X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33475336209479073\n",
      "0.33464059710197225\n"
     ]
    }
   ],
   "source": [
    "xloc = X_test[0:1]\n",
    "gradient = difference_gradient(fmodel,xloc,sds)\n",
    "hessian = difference_hessian(fmodel,xloc,sds)\n",
    "\n",
    "shap_CV_true_indep = compute_true_shap_cv_indep(xloc, gradient, hessian, feature_means, cov_mat, mapping_dict=mapping_dict)\n",
    "sum_shap_CV_true = np.sum(shap_CV_true_indep)\n",
    "avg_CV_empirical = np.mean(f_second_order_approx(fmodel(xloc),X_train, xloc, gradient, hessian))\n",
    "pred = fmodel(xloc)#[0]\n",
    "exp_CV_sum_empirical = (pred - avg_CV_empirical)[0]\n",
    "print(sum_shap_CV_true)\n",
    "print(exp_CV_sum_empirical)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapley Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32. 31. 74. 65. 49. 45. 29. 22. 26. 60. 28. 53. 56. 27. 34.  6.  6.  0.\n",
      " 22. 60.]\n"
     ]
    }
   ],
   "source": [
    "independent_features = True\n",
    "obj_ss = cv_shapley_sampling(fmodel, X_train, xloc, \n",
    "                        independent_features,\n",
    "                        gradient, hessian,\n",
    "                        mapping_dict=mapping_dict,\n",
    "                        M=100, n_samples_per_perm=10) # M is number of permutations\n",
    "final_ests, vshap_ests_model, vshap_ests_CV, corr_ests = obj_ss\n",
    "\n",
    "order = np.argsort(np.abs(final_ests))[::-1]\n",
    "print(np.round(100*(np.maximum(corr_ests, 0)**2)[order])) # Variance reductions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KernelSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45. 35. 47. 28. 32. 40. 43. 31. 27. 41. 42. 34. 31. 37. 35. 42. 36. 37.\n",
      " 36. 33.]\n"
     ]
    }
   ],
   "source": [
    "independent_features = True\n",
    "obj_kshap = cv_kshap(fmodel, X_train, xloc, \n",
    "            independent_features,\n",
    "            gradient, hessian,\n",
    "            mapping_dict=mapping_dict,var_method=\"ls\",\n",
    "            M=1000, n_samples_per_perm=10)\n",
    "final_ests, vshap_ests_model, vshap_ests_CV, corr_ests = obj_kshap\n",
    "\n",
    "order = np.argsort(np.abs(final_ests))[::-1]\n",
    "print(np.round(100*(np.maximum(corr_ests,0)**2)[order])) # Variance reductions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ControlSHAP values, assuming dependent features\n",
    "First, prepare for dependent sampling by precomputing matrices. Note that we use more permutations to estimate the D matrices, so as to obtain a reliable estimate of $\\phi^\\text{approx}(x)$. While this computation is somewhat expensive, we can reuse the resulting matrices for as many local points $x$ as we like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for dependent sampling\n",
    "M_linear = 500\n",
    "D_matrices = make_all_lundberg_matrices(M_linear, cov2)\n",
    "shap_CV_true_dep = linear_shap_vals(xloc, D_matrices, feature_means, gradient, mapping_dict=mapping_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapley Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71. 74. 78. 74. 70. 69. 73. 74. 69. 82. 85. 85. 80. 79. 73. 79. 80. 80.\n",
      " 80. 74.]\n"
     ]
    }
   ],
   "source": [
    "independent_features = False\n",
    "obj_dep = cv_shapley_sampling(fmodel, X_train, xloc,\n",
    "                    independent_features,\n",
    "                    gradient,\n",
    "                    mapping_dict=mapping_dict,\n",
    "                    shap_CV_true=shap_CV_true_dep, # Equivalently, can give D_matrices instead\n",
    "                    M=100,n_samples_per_perm=10,\n",
    "                    cov_mat=cov2)\n",
    "final_ests, vshap_ests_model, vshap_ests_CV, corr_ests = obj_dep\n",
    "order = np.argsort(np.abs(final_ests))[::-1]\n",
    "print(np.round(100*(np.maximum(corr_ests,0)**2)[order])) # Variance reductions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  1 11  8  4  3 19 14  7  5 13  9 17 12 16 10  6 15 18  0]\n",
      "[ 2  1 11  0 19 12  7 13 10 14  6  4  8 16 17  9  3 18  5 15]\n",
      "[ 2 11 19  6  1 13  8  3 16 14  7 17 15  4  9 10 12  0  5 18]\n"
     ]
    }
   ],
   "source": [
    "print(np.argsort(np.abs(vshap_ests_model))[::-1])\n",
    "print(np.argsort(np.abs(final_ests))[::-1])\n",
    "print(np.argsort(np.abs(shap_CV_true_dep))[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KernelSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16. 34. 36. 60. 36. 28. 47. 16. 28. 40. 34. 43. 67. 41. 19. 37. 55. 25.\n",
      " 69. 45.]\n"
     ]
    }
   ],
   "source": [
    "independent_features = False\n",
    "obj_kshap_dep = cv_kshap(fmodel, X_train, xloc,\n",
    "                    independent_features,\n",
    "                    gradient,\n",
    "                    mapping_dict=mapping_dict,\n",
    "                    shap_CV_true=shap_CV_true_dep,\n",
    "                    M=100,n_samples_per_perm=10, var_method=\"ls\",\n",
    "                    cov_mat=cov2)\n",
    "final_ests, vshap_ests_model, vshap_ests_CV, corr_ests = obj_kshap_dep\n",
    "order = np.argsort(np.abs(final_ests))[::-1]\n",
    "print(np.round(100*(np.maximum(corr_ests,0)**2)[order])) # Variance reductions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "768407c71a286f507fab4bce553d71b5cbd766c247b76eb598ef769225202bc3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.16 ('shap')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
