{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeremygoldwasser/opt/anaconda3/envs/shap/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../HelperFiles/')\n",
    "from helper import *\n",
    "from helper_dep import *\n",
    "from helper_indep import *\n",
    "from helper_shapley_sampling import *\n",
    "from helper_kshap import *\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_path = \"../Simulations/Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BRCA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "np.random.seed(1)\n",
    "data = pd.read_csv(join(data_path, \"brca_small.csv\"))\n",
    "X = data.values[:, :-1][:,:20]\n",
    "Y = data.values[:, -1]\n",
    "Y = (Y==2).astype(int) # Formulate as binary classification problem\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=100, random_state=0)\n",
    "\n",
    "# Normalize\n",
    "mean = X_train.mean(axis=0)\n",
    "std = X_train.std(axis=0)\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std\n",
    "d = X_train.shape[1]\n",
    "mapping_dict = None\n",
    "\n",
    "# Compute mean and covariance of training data\n",
    "feature_means = np.mean(X_train, axis=0)\n",
    "cov_mat = np.cov(X_train, rowvar=False)\n",
    "cov2 = correct_cov(cov_mat) # Recondition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.6165\n",
      "Epoch 2/5, Loss: 0.6549\n",
      "Epoch 3/5, Loss: 0.5868\n",
      "Epoch 4/5, Loss: 0.5764\n",
      "Epoch 5/5, Loss: 0.5998\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.tanh(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n",
    "# Convert the input and label data to PyTorch tensors\n",
    "inputs = torch.tensor(X_train, dtype=torch.float32)\n",
    "labels = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "# Compute the class weights\n",
    "class_counts = torch.bincount(labels)\n",
    "num_samples = len(labels)\n",
    "class_weights = 1.0 / class_counts.float()\n",
    "sample_weights = class_weights[labels]\n",
    "\n",
    "# Create a sampler with balanced weights\n",
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=num_samples, replacement=True)\n",
    "\n",
    "# Create a DataLoader with the sampler\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Create an instance\n",
    "net = TwoLayerNet(input_size=d, hidden_size=50, output_size=2)\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()#weight=torch.tensor(weights)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)#.01\n",
    "\n",
    "# Iterate over the training data in batches\n",
    "num_epochs = 5\n",
    "\n",
    "# Train the network for the specified number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in dataloader:\n",
    "        # Zero the gradients for this batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute the forward pass of the network\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # Compute the loss for this batch\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Compute the gradients of the loss with respect to the parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters using the optimizer\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move to numpy & evaluate predictive accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class imbalance: 55%\n",
      "NN with balanced sampling: 73% accuracy\n"
     ]
    }
   ],
   "source": [
    "def neural_net(x):\n",
    "    output = net(x)[0,1] if x.shape[0]==1 else net(x)[:,1]\n",
    "    return output\n",
    "\n",
    "def compute_hessian(x):\n",
    "    if not torch.is_tensor(x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "    hessian = torch.autograd.functional.hessian(neural_net, x)\n",
    "    hessian = hessian.reshape((d,d)).detach().numpy()\n",
    "    return hessian\n",
    "\n",
    "def fmodel(x):\n",
    "    if not torch.is_tensor(x):\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "    return neural_net(x).detach().numpy()\n",
    "\n",
    "\n",
    "print(\"Class imbalance: {}%\".format(round(100*(max(np.mean(y_test), 1-np.mean(y_test))))))\n",
    "Y_preds = (fmodel(X_test) > 0.5).astype(\"int\")\n",
    "print(\"NN with balanced sampling: {}% accuracy\".format(round(np.mean(Y_preds == y_test)*100)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute CV-SHAP values, assuming independent features\n",
    "### Compute true SHAP values of Taylor approximation around $x$, and verify $\\sum_{j=1}^d \\phi_j(x) \\approx f(x)-Ef(X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradient and hessian around local x\n",
    "xloc = X_test[0:1]\n",
    "xloc_torch = torch.tensor(xloc, dtype=torch.float32).requires_grad_(True)\n",
    "y_pred = net(xloc_torch)[0,1]\n",
    "y_pred.backward()\n",
    "gradient = xloc_torch.grad.detach().numpy().reshape((d, 1))\n",
    "hessian = compute_hessian(xloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.34031945\n",
      "-0.34014217331407215\n"
     ]
    }
   ],
   "source": [
    "# Obtain true SHAP values and verify their feasibility\n",
    "shap_CV_true_indep = compute_true_shap_cv_indep(xloc, gradient, hessian, feature_means, cov_mat, mapping_dict=mapping_dict)\n",
    "sum_shap_CV_true = np.sum(shap_CV_true_indep)\n",
    "avg_CV_empirical = np.mean(f_second_order_approx(fmodel(xloc),X_train, xloc, gradient, hessian))\n",
    "pred = fmodel(xloc)#[0]\n",
    "exp_CV_sum_empirical = pred - avg_CV_empirical\n",
    "print(sum_shap_CV_true)\n",
    "print(exp_CV_sum_empirical)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapley Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75. 79. 86. 73. 80. 71. 74. 89. 83. 80. 66. 75. 38. 91. 47. 94. 86.  0.\n",
      " 56. 53.]\n"
     ]
    }
   ],
   "source": [
    "independent_features = True\n",
    "obj_ss = cv_shapley_sampling(fmodel, X_train, xloc, \n",
    "                        independent_features,\n",
    "                        gradient, hessian,\n",
    "                        mapping_dict=mapping_dict,\n",
    "                        M=100, n_samples_per_perm=10) # M is number of permutations\n",
    "final_ests, vshap_ests_model, vshap_ests_CV, corr_ests = obj_ss\n",
    "\n",
    "order = np.argsort(np.abs(final_ests))[::-1]\n",
    "print(np.round(100*(np.maximum(corr_ests, 0)**2)[order])) # Variance reductions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare predicted rankings. ControlSHAP estimates are slightly closer to those of the control variate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14  9  6  5 18  8 19  4 11 12  2 15 13  0 17  1  7 16 10  3]\n",
      "[14  9  6  5  8 18 19  4 11 12  2 15 13  0  1 17  7 16 10  3]\n",
      "[14  9  5 19  6 18  8  4 13 12  2 11  0 15 16  7  1 17 10  3]\n"
     ]
    }
   ],
   "source": [
    "print(np.argsort(np.abs(vshap_ests_model))[::-1])\n",
    "print(np.argsort(np.abs(final_ests))[::-1])\n",
    "print(np.argsort(np.abs(shap_CV_true_indep).reshape(-1))[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KernelSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90. 83. 86. 90. 90. 84. 88. 72. 77. 87. 84. 84. 81. 90. 84. 87. 83. 86.\n",
      " 78. 88.]\n"
     ]
    }
   ],
   "source": [
    "independent_features = True\n",
    "obj_kshap = cv_kshap(fmodel, X_train, xloc, \n",
    "            independent_features,\n",
    "            gradient, hessian,\n",
    "            mapping_dict=mapping_dict,var_method=\"ls\",\n",
    "            M=1000, n_samples_per_perm=10)\n",
    "final_ests, vshap_ests_model, vshap_ests_CV, corr_ests = obj_kshap\n",
    "\n",
    "order = np.argsort(np.abs(final_ests))[::-1]\n",
    "print(np.round(100*(np.maximum(corr_ests,0)**2)[order])) # Variance reductions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute CV-SHAP values, assuming dependent features\n",
    "First, prepare for dependent sampling by precomputing matrices. Note that we use more permutations to estimate the D matrices, so as to obtain a reliable estimate of $\\phi^\\text{approx}(x)$. While this computation is somewhat expensive, we can reuse the resulting matrices for as many local points $x$ as we like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_linear = 5000\n",
    "D_matrices = make_all_lundberg_matrices(M_linear, cov2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapley Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85. 83. 64. 85. 85. 85. 85. 84. 89. 93. 79. 86. 83. 87. 87. 86. 88. 89.\n",
      " 86. 92.]\n"
     ]
    }
   ],
   "source": [
    "independent_features = False\n",
    "shap_CV_true_dep = linear_shap_vals(xloc, D_matrices, feature_means, gradient, mapping_dict=mapping_dict)\n",
    "obj_dep = cv_shapley_sampling(fmodel, X_train, xloc,\n",
    "                    independent_features,\n",
    "                    gradient,\n",
    "                    mapping_dict=mapping_dict,\n",
    "                    shap_CV_true=shap_CV_true_dep, # Equivalently, can give D_matrices instead\n",
    "                    M=100,n_samples_per_perm=10,\n",
    "                    cov_mat=cov2)\n",
    "final_ests, vshap_ests_model, vshap_ests_CV, corr_ests = obj_dep\n",
    "order = np.argsort(np.abs(final_ests))[::-1]\n",
    "print(np.round(100*(np.maximum(corr_ests,0)**2)[order])) # Variance reductions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14 19 18  4  8  9  5 11  6 12 17 10  2  3 13  1 15  7  0 16]\n",
      "[14 19 18  4  8  9  5 11  6 12 17  2 15  0 10  3  7  1 13 16]\n",
      "[19 14 18  9  4  8  5 12 11 13 17  6 16 15  1  2 10  0  3  7]\n"
     ]
    }
   ],
   "source": [
    "print(np.argsort(np.abs(vshap_ests_model))[::-1])\n",
    "print(np.argsort(np.abs(final_ests))[::-1])\n",
    "print(np.argsort(np.abs(shap_CV_true_dep))[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KernelSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85. 91. 93. 85. 92. 93. 91. 88. 92. 93. 90. 91. 91. 91. 93. 94. 89. 91.\n",
      " 93. 94.]\n"
     ]
    }
   ],
   "source": [
    "independent_features = False\n",
    "obj_kshap_dep = cv_kshap(fmodel, X_train, xloc,\n",
    "                    independent_features,\n",
    "                    gradient,\n",
    "                    mapping_dict=mapping_dict,\n",
    "                    shap_CV_true=shap_CV_true_dep,\n",
    "                    M=500,n_samples_per_perm=10, var_method=\"ls\",\n",
    "                    cov_mat=cov2)\n",
    "final_ests, vshap_ests_model, vshap_ests_CV, corr_ests = obj_kshap_dep\n",
    "order = np.argsort(np.abs(final_ests))[::-1]\n",
    "print(np.round(100*(np.maximum(corr_ests,0)**2)[order])) # Variance reductions"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "768407c71a286f507fab4bce553d71b5cbd766c247b76eb598ef769225202bc3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.16 ('shap')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
