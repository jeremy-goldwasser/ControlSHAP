{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeremygoldwasser/opt/anaconda3/envs/shap/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import shap\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from helper import *\n",
    "from helper_dep import *\n",
    "from helper_indep import *\n",
    "from helper_shapley_sampling import *\n",
    "from helper_kshap import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "German Credit dataset has 1000 samples, 20 covariates; response is \"good customer\" (binary); ~70% of Ys are 1. On UCI ML Repo.\n",
    "\n",
    "Problem: can't load as already categorical. The Gradient Boosting model they used allowed you to just input the (numerical) data w/ a list of the categorical indices -- we can't do that with sklrean logistic regression.\n",
    "- FWIW, I think we had to convert things manually for the bank dataset. I just don't want to have to deal with that again.\n",
    "- Actually, it might have been OK to begin with - I just made things more complicated than necessary. Wouldn't be the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sage \n",
    "# df = sage.datasets.credit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Census dataset has 30k samples, 12 covariates, binary response, ~75% of Ys are False; some features numerical, some categorical.\n",
    "\n",
    "From UCI ML Repository. Predict whether income exceeds $50K/yr based on census data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 12)\n",
      "(array([False,  True]), array([24720,  7841]))\n",
      "(32561, 91)\n"
     ]
    }
   ],
   "source": [
    "X, y = shap.datasets.adult()\n",
    "X_display, y_display = shap.datasets.adult(display=True)\n",
    "\n",
    "print(X.shape)\n",
    "X.head()\n",
    "X_display.head() # No NAs\n",
    "print(np.unique(y_display, return_counts=True))\n",
    "X_binarized = pd.get_dummies(X_display)\n",
    "print(X_binarized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make dictionary whose keys are all the original columns, & whose values are lists of all (could just be 1) the columns in the binarized dataset with those features.\n",
    "- This will be useful when we get around to fitting the SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {}\n",
    "for i, col in enumerate(X_display.columns):\n",
    "    bin_cols = []\n",
    "    for j, bin_col in enumerate(X_binarized.columns):\n",
    "        if bin_col.startswith(col):\n",
    "            bin_cols.append(j)\n",
    "    mapping_dict[i] = bin_cols\n",
    "# mapping_dict\n",
    "# print(np.sum(np.sum(X_train[:, 14:21], axis=1) != 1)) # Sanity check: It's right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescale covariates to be between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = (X_binarized-X_binarized.min())/(X_binarized.max()-X_binarized.min())\n",
    "y_int = y_display.astype(\"int8\")\n",
    "\n",
    "# Split into training & test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "n, d_bin = X_norm.shape\n",
    "n_train = round(n*0.75)\n",
    "train_idx = np.random.choice(n, size=n_train, replace=False)\n",
    "# X_train, y_train = X_norm.iloc[train_idx], y_int[train_idx]\n",
    "X_train_pd, y_train = X_norm.iloc[train_idx], y_int[train_idx]\n",
    "X_train = X_train_pd.to_numpy()\n",
    "\n",
    "test_idx = np.setdiff1d(list(range(n)), train_idx)\n",
    "X_test_pd, y_test = X_norm.iloc[test_idx], y_int[test_idx]\n",
    "X_test = X_test_pd.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train logistic regression model\n",
    "\n",
    "85% test accuracy, compared w/ 75% class imbalance. So model is pretty good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
    "print(round(np.mean(logreg.predict(X_test)==y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreate logreg model in numpy\n",
    "## NOTE: Previous iterations didn't have (or need) an intercept term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1615208 0.8384792]]\n",
      "0.8384791974975092\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "BETA = logreg.coef_.reshape(d_bin)\n",
    "INTERCEPT = logreg.intercept_\n",
    "\n",
    "def model(x):\n",
    "    yhat = sigmoid(np.dot(x, BETA) + INTERCEPT)\n",
    "    return yhat.item() if x.shape[0]==1 else yhat\n",
    "\n",
    "xloc = X_test[0:1]\n",
    "print(logreg.predict_proba(X_test[0:1]))\n",
    "print(model(xloc)) # Yes, our function matches sklearn\n",
    "print(y_test[0]) # Correct classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute gradient & hessian wrt local x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = logreg_gradient(model, xloc, BETA)\n",
    "hessian = logreg_hessian(model, xloc, BETA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute SHAP values, assuming independent features\n",
    "#### Sanity check: Verify true SHAP values of the quadratic approximation add up to $f(x)-Ef(X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.513460422903776\n",
      "1.5134465875411895\n"
     ]
    }
   ],
   "source": [
    "feature_means = np.mean(X_train, axis=0)\n",
    "cov_mat = np.cov(X_train, rowvar=False)\n",
    "\n",
    "avg_CV_empirical = np.mean(f_second_order_approx(model,X_train, xloc, gradient, hessian))\n",
    "pred = model(xloc)\n",
    "exp_CV_sum_empirical = pred - avg_CV_empirical\n",
    "shap_CV_true_indep = compute_true_shap_cv_indep(xloc, gradient, hessian, feature_means, cov_mat, mapping_dict=mapping_dict)\n",
    "sum_shap_CV_true = np.sum(shap_CV_true_indep)\n",
    "print(sum_shap_CV_true)\n",
    "print(exp_CV_sum_empirical) # Yes, they're extremely close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapley Sampling\n",
    "### Bad/weird. Negative correlations for the important features; positive for unimportant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.22056109  0.1186138   0.11724608  0.1138988   0.04962156 -0.03434943\n",
      "  0.00967933  0.00869379 -0.00772812  0.00543945 -0.00340953  0.00245303]\n",
      "[-0.27 -0.19 -0.49 -0.39 -0.02  0.45  0.16  0.57  0.78  0.44  0.61  0.45]\n",
      "[ 7.  4. 24. 15.  0. 20.  3. 32. 61. 19. 37. 20.]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(13)\n",
    "independent_features = True\n",
    "obj_ss = cv_shapley_sampling(model, X_train, xloc, \n",
    "                        independent_features,\n",
    "                        gradient, hessian,\n",
    "                        mapping_dict=mapping_dict,\n",
    "                        M=100, n_samples_per_perm=10) # M is number of permutations\n",
    "final_ests, vshap_ests_model, vshap_ests_CV, corr_ests = obj_ss\n",
    "\n",
    "order = np.argsort(np.abs(final_ests))[::-1]\n",
    "print(final_ests[order]) # Final SHAP estimates, ordered\n",
    "print(np.round(corr_ests[order], 2)) # Correlations\n",
    "print(np.round(100*(corr_ests**2)[order])) # Variance reductions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KernelSHAP\n",
    "#### Now things look good. 30-50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.129  0.077  0.052  0.051  0.042  0.027 -0.018 -0.017  0.009  0.008\n",
      "  0.002  0.001]\n",
      "[0.56 0.66 0.72 0.74 0.47 0.7  0.63 0.67 0.64 0.7  0.75 0.55]\n",
      "[32. 43. 52. 55. 22. 49. 40. 45. 41. 49. 57. 31.]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "obj_kshap = cv_kshap(model, X_train, xloc, \n",
    "            independent_features,\n",
    "            gradient, hessian,\n",
    "            mapping_dict=mapping_dict,\n",
    "            M=1000, n_samples_per_perm=10, \n",
    "            var_method='boot', n_boot=250)\n",
    "final_ests, vshap_ests_model, vshap_ests_CV, corr_ests = obj_kshap\n",
    "order = np.argsort(np.abs(final_ests))[::-1]\n",
    "print(np.round(final_ests[order], 3)) # Final SHAP estimates, ordered\n",
    "print(np.round(corr_ests[order], 2)) # Correlations\n",
    "print(np.round(100*(corr_ests**2)[order])) # Variance reductions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependent Features\n",
    "### Recondition covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.428543530227902e+16\n"
     ]
    }
   ],
   "source": [
    "u, s, vh = np.linalg.svd(cov_mat, full_matrices=True)\n",
    "print(s[0]/s[d_bin-1]) # Conditioning number is essentially infinite\n",
    "s_max = s[0]\n",
    "K = 10000 # Desired conditioning number\n",
    "min_acceptable = s_max/K\n",
    "s2 = np.copy(s)\n",
    "s2[s <= min_acceptable] = min_acceptable\n",
    "cov2 = np.matmul(u, np.matmul(np.diag(s2), vh))\n",
    "# Basically identical\n",
    "# print(cov2[:3,:3])\n",
    "# print(cov_mat[:3,:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate matrices to estimate true SHAP values\n",
    "- Takes 1m 15s for 1000 perms. 91x91 matrix so takes a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D_matrices = make_all_lundberg_matrices(1000, cov2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapley Sampling\n",
    "Variance reductions around mid-90s!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "covariance is not positive-semidefinite.\n",
      "covariance is not positive-semidefinite.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1645815  0.10720158 0.04594794 0.04562969 0.04204151 0.02953907\n",
      " 0.02329357 0.02262578 0.02258273 0.02033402 0.01488499 0.00935861]\n",
      "[0.96 0.95 0.97 0.99 0.92 0.98 0.98 0.97 0.97 0.98 0.96 0.94]\n",
      "[93. 89. 94. 97. 84. 95. 97. 94. 93. 96. 93. 88.]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "independent_features = False\n",
    "shap_CV_true_dep = linear_shap_vals(xloc, D_matrices, feature_means, gradient)\n",
    "obj_dep = cv_shapley_sampling(model, X_train, xloc,\n",
    "                    independent_features,\n",
    "                    gradient,\n",
    "                    shap_CV_true=shap_CV_true_dep, # Equivalently, can give D_matrices instead\n",
    "                    M=100,n_samples_per_perm=10,\n",
    "                    mapping_dict=mapping_dict,\n",
    "                    cov_mat=cov2)\n",
    "final_ests, vshap_ests_model, vshap_ests_CV, corr_ests = obj_dep\n",
    "order = np.argsort(np.abs(final_ests))[::-1]\n",
    "print(final_ests[order]) # Final SHAP estimates, ordered\n",
    "print(np.round(corr_ests[order], 2)) # Correlations\n",
    "print(np.round(100*(corr_ests**2)[order])) # Variance reductions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KernelSHAP\n",
    "Again, around 90% variance reduction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "covariance is not positive-semidefinite.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.08526138  0.08317849  0.02560378  0.01684514 -0.01227594 -0.01215819\n",
      "  0.01103153  0.00620537  0.00499942  0.00401419  0.00301522  0.00197274]\n",
      "[0.88 0.92 0.89 0.93 0.95 0.83 0.92 0.91 0.91 0.92 0.94 0.93]\n",
      "[78. 85. 79. 86. 90. 69. 84. 84. 82. 85. 88. 87.]\n"
     ]
    }
   ],
   "source": [
    "%run helper_dep\n",
    "np.random.seed(1)\n",
    "independent_features = False\n",
    "shap_CV_true_dep = linear_shap_vals(xloc, D_matrices, feature_means, gradient)\n",
    "obj_kshap_dep = cv_kshap(model, X_train, xloc,\n",
    "                    independent_features,\n",
    "                    gradient,\n",
    "                    shap_CV_true=shap_CV_true_dep,\n",
    "                    M=1000,n_samples_per_perm=10,\n",
    "                    mapping_dict=mapping_dict,\n",
    "                    cov_mat=cov2)\n",
    "\n",
    "final_ests, vshap_ests_model, vshap_ests_CV, corr_ests = obj_kshap_dep\n",
    "order = np.argsort(np.abs(final_ests))[::-1]\n",
    "print(final_ests[order]) # Final SHAP estimates, ordered\n",
    "print(np.round(corr_ests[order], 2)) # Correlations\n",
    "print(np.round(100*(corr_ests**2)[order])) # Variance reductions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "768407c71a286f507fab4bce553d71b5cbd766c247b76eb598ef769225202bc3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.16 ('shap')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
